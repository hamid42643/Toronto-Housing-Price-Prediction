{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxYMFoX/JNNbV90wUGwgWL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamid42643/Toronto-Housing-Price-Prediction/blob/main/JFriedel_Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BTytTetK0Q_H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESkVGLKaz76_"
      },
      "outputs": [],
      "source": [
        "Assignment 4.1: Making the Cut\n",
        "Due Monday by 11:59pm Points 90 Submitting a text entry box, a website url, a media recording, or a file upload Attempts 0 Allowed Attempts 3\n",
        "Introduction\n",
        "In this assignment, you will leverage k-means clustering to create groupings from the credit default data, experiment with different parameters available\n",
        "in the model (\"k\" and distance metrics), and interpret the output from a business perspective.\n",
        "\n",
        "Instructions\n",
        "Create a k-means model with the assignment dataset using at least 10 features.\n",
        "Experiment with at least 3 k values. Be sure to transform variables into the appropriate format before modeling. Note that a larger k will increase the\n",
        "overhead of interpretation, so it is suggested to keep the k less than 10.\n",
        "What transformations did you apply to the raw dataset?\n",
        "What were different k's chosen? What were the differences in the output with those different k's?\n",
        "Choose a final k that you think reflects the data the best and provide a written interpretation of the different clusters generated by k-means\n",
        "Why did you choose this k and distance metric?\n",
        "Why does it appear these groups have been created? What are the influential features?\n",
        "Are there any inferences you can draw that would be relevant from a business context about the different groups?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading the required libraries\n",
        "\n",
        "import numpy as np # Pandas for array manipulation\n",
        "import pandas as pd # Pandas for data manipulation\n",
        "import seaborn as sns # Seaborn for visualizing. Note: we will also use one of the data sets\n",
        "import matplotlib.pyplot as plt # Matplotlib for subplots\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler # Import for feature standardization\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score # For kmeans evaluation\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "WaiWcvnG0hWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df\n",
        "df = load_dataset(\"train_data.csv\")"
      ],
      "metadata": {
        "id": "ECQ1IXsz0sel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree # Import Tree Classifiers\n",
        "from sklearn.ensemble import RandomForestClassifier # Import Random Forest Classifiers\n",
        "from sklearn.model_selection import train_test_split,  RandomizedSearchCV # Import train_test_split function\n",
        "from sklearn import metrics # Import scikit-learn metrics module for accuracy calculation\n",
        "from scipy.stats import randint # Generate random numbers\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "z-ItU2SP02qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Processing"
      ],
      "metadata": {
        "id": "3eewuE3V1H54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap = df.shape\n",
        "print(\"Shape of the dataframe (row, col):\",shap,\"\\r\\n\")"
      ],
      "metadata": {
        "id": "ssY4LkZn1MVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "-gJm-fSr1TO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values with 0\n",
        "df = df.fillna(0)"
      ],
      "metadata": {
        "id": "1oSuo3rT1bT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert enuerations to integer\n",
        "del df['OCCUPATION_TYPE']\n",
        "del df['ORGANIZATION_TYPE']\n",
        "df[\"HOUSETYPE_MODE\"] = df['HOUSETYPE_MODE'].replace({'block of flats': 1, 'terraced house': 2, 'specific housing': 3}).astype(int)\n",
        "df[\"FONDKAPREMONT_MODE\"] = df['FONDKAPREMONT_MODE'].replace({'reg oper account': 1, 'org spec account': 2, 'reg oper spec account': 3, 'not specified': 4}).astype(int)\n",
        "df[\"NAME_HOUSING_TYPE\"] = df['NAME_HOUSING_TYPE'].replace({'House / apartment': 1, 'Municipal apartment': 2, 'Office apartment': 3, 'Rented apartment': 4, 'With parents': 5, 'Co-op apartment':6}).astype(int)\n",
        "df[\"NAME_EDUCATION_TYPE\"] = df['NAME_EDUCATION_TYPE'].replace({'Secondary / secondary special': 1, 'Higher education': 2, 'Lower secondary': 3, 'Incomplete higher': 4, 'Academic degree': 5}).astype(int)\n",
        "df[\"NAME_INCOME_TYPE\"] = df['NAME_INCOME_TYPE'].replace({'Working': 1, 'Pensioner': 2, 'Commercial associate': 3, 'State servant': 4, 'Unemployed': 5, 'Businessman': 6, 'Student': 7, 'Maternity leave': 7}).astype(int)\n",
        "df[\"NAME_TYPE_SUITE\"] = df['NAME_TYPE_SUITE'].replace({'Unaccompanied': 1, 'Family': 2, 'Children': 3, 'Spouse, partner': 4, 'Other_A': 5, 'Other_B': 6, 'Group of people': 7}).astype(int)\n",
        "df[\"NAME_FAMILY_STATUS\"] = df['NAME_FAMILY_STATUS'].replace({'Married': 1, 'Single / not married': 2, 'Civil marriage': 3, 'Widow': 4, 'Separated': 5, 'Unknown': 6, 'Others': 7}).astype(int)\n",
        "df[\"WALLSMATERIAL_MODE\"] = df['WALLSMATERIAL_MODE'].replace({'Panel': 1, 'Wooden': 2, 'Stone, brick': 3, 'Block': 4, 'Mixed': 5, 'Monolithic': 6, 'Others': 7, 'Unknown': 8}).astype(int)\n",
        "df[\"EMERGENCYSTATE_MODE\"] = df['EMERGENCYSTATE_MODE'].replace({'No': 1, 'Yes': 2}).astype(int)\n",
        "df[\"FLAG_OWN_CAR\"] = df['FLAG_OWN_CAR'].replace({'N': 0, 'Y': 1}).astype(int)\n",
        "df[\"FLAG_OWN_REALTY\"] = df['FLAG_OWN_REALTY'].replace({'N': 0, 'Y': 1}).astype(int)\n",
        "df[\"CODE_GENDER\"] = df['CODE_GENDER'].replace({'M': 0, 'F': 1}).astype(int)\n",
        "df[\"NAME_CONTRACT_TYPE\"] = df['NAME_CONTRACT_TYPE'].replace({'Cash loans': 0, 'Revolving loans': 1}).astype(int)\n",
        "df[\"WEEKDAY_APPR_PROCESS_START\"] = df['WEEKDAY_APPR_PROCESS_START'].replace({'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7}).astype(int)\n",
        "#display the first 5 rows in dataset:\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "AE_Gsp-41lFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.TARGET"
      ],
      "metadata": {
        "id": "6OPpOxbP1u_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=pd.DataFrame(df)"
      ],
      "metadata": {
        "id": "aUSBpVji10Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features\n",
        "features = ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'AMT_ANNUITY', 'DAYS_EMPLOYED', 'OWN_CAR_AGE', 'NAME_FAMILY_STATUS']\n",
        "#features = ['AMT_INCOME_TOTAL']\n",
        "X_selected = X[features]"
      ],
      "metadata": {
        "id": "k8IAmHzN147u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Target\n",
        "y = X['TARGET']"
      ],
      "metadata": {
        "id": "PQFyB4a82De3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target=df.TARGET"
      ],
      "metadata": {
        "id": "VfkUq5PA2ImO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All features are continuous values but on different scales. Need to scale data in order to prevent larger magnitude columns from taking over the model. Look at the raw distribution of each feature, and then the means of all features"
      ],
      "metadata": {
        "id": "NZlHfjWN2WHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "means = []\n",
        "for col in df.columns:\n",
        "    print(f'Producing stats for column: {col}')\n",
        "    print(df[col].describe([0.1, 0.25, 0.5, 0.75, 0.9]))\n",
        "    means.append([col,df[col].describe()['mean']])"
      ],
      "metadata": {
        "id": "SKxZi-gm2tsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distinct different in mean across the different variables. Need to scale data to accommodate for the k-means algorithm"
      ],
      "metadata": {
        "id": "pYbTFde23GLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(means, columns = ['column', 'mean'])"
      ],
      "metadata": {
        "id": "yIvwEdSm22Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "v1LZ--8a3bU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(df.columns[14:122], axis=1)"
      ],
      "metadata": {
        "id": "VqahC2Lw3g2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap = df.shape\n",
        "print(\"Shape of the dataframe (row, col):\",shap,\"\\r\\n\")"
      ],
      "metadata": {
        "id": "NOkkK9wQ3inI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#summarize the datasetâ€™s statistical properties, such as count, mean, min, and max:\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "JyWLECdo3xxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the scaled data\n",
        "\n",
        "sns.pairplot(df) # Plot each variable against all other variables\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y7m8rEPr3il2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's scale the data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "scaled_df = pd.DataFrame(scaled_data, columns = df.columns)"
      ],
      "metadata": {
        "id": "6IYaYUmS4LGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's evaluate the scaled data in the same fashion\n",
        "\n",
        "scaled_means = []\n",
        "for col in scaled_df.columns:\n",
        "    print(f'Producing stats for column: {col}')\n",
        "    print(scaled_df[col].describe([0.1, 0.25, 0.5, 0.75, 0.9]))\n",
        "    scaled_means.append([col,scaled_df[col].describe()['mean']])"
      ],
      "metadata": {
        "id": "6itQtvpB4Nzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(scaled_means, columns = ['column', 'mean'])"
      ],
      "metadata": {
        "id": "Dvx0P-_D4UX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the scaled data\n",
        "# You can see the variable relationships are maintained but the magnitude of the variables are different relative to the raw data\n",
        "\n",
        "sns.pairplot(scaled_df) # Plot each variable against all other variables\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MSmk7T664ceL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Clustering\n",
        "\n",
        "Run multiple variations of K-means with different variations of k-means, from 3-6. Evaluate the output - Evaluate by joining the raw data back with the cluster assignments, including the target variable available in the dataset, then looking at how the clusters map to the raw data."
      ],
      "metadata": {
        "id": "0iR5O6QH4hm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OMP_NUM_THREADS=1"
      ],
      "metadata": {
        "id": "hKQep-kJ4yRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_execution(df, num_clust):\n",
        "    # Create models & store labels for different number of clusters\n",
        "    kmn = KMeans(n_clusters = num_clust, n_init='auto', random_state = 0) # Create KMeans object\n",
        "    kmn.fit(df) # Apply to the data\n",
        "    kmn_lbl = kmn.labels_ # Capture K-Means labels\n",
        "    kmn_distortion = kmn.inertia_ # Used for elbow method\n",
        "    return kmn, kmn_lbl, kmn_distortion\n",
        "\n",
        "#kmn_2_mod, kmn_2_labels, kmn_2_dist = kmeans_execution(scaled_df, 2)\n",
        "#silhouette_2 = silhouette_score(scaled_df, kmn_2_labels)\n",
        "#print(f'k=2 silhouette average score: {silhouette_2}')\n",
        "kmn_3_mod, kmn_3_labels, kmn_3_dist = kmeans_execution(scaled_df, 3)\n",
        "silhouette_3 = silhouette_score(scaled_df, kmn_3_labels)\n",
        "print(f'k=3 silhouette average score: {silhouette_3}')\n",
        "#kmn_4_mod, kmn_4_labels, kmn_4_dist = kmeans_execution(scaled_df, 4)\n",
        "#silhouette_4 = silhouette_score(scaled_df, kmn_4_labels)\n",
        "#print(f'k=4 silhouette average score: {silhouette_4}')\n",
        "kmn_5_mod, kmn_5_labels, kmn_5_dist = kmeans_execution(scaled_df, 5)\n",
        "silhouette_5 = silhouette_score(scaled_df, kmn_5_labels)\n",
        "print(f'k=5 silhouette average score: {silhouette_5}')\n",
        "kmn_6_mod, kmn_6_labels, kmn_6_dist = kmeans_execution(scaled_df, 6)\n",
        "silhouette_6 = silhouette_score(scaled_df, kmn_6_labels)\n",
        "print(f'k=6 silhouette average score: {silhouette_6}')"
      ],
      "metadata": {
        "id": "9IvdJA6w43NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elbow Plot - seems like clear \"elbow\" at k=4\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "#plt.plot(range(2, 7), [kmn_2_dist, kmn_3_dist, kmn_4_dist, kmn_5_dist, kmn_6_dist])\n",
        "plt.plot(range(3, 6), [kmn_3_dist, kmn_5_dist, kmn_6_dist])\n",
        "plt.grid(True)\n",
        "plt.title('Elbow curve')"
      ],
      "metadata": {
        "id": "M8TtRkWx5BrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create evaluation dataset\n",
        "\n",
        "eval_df = df.copy()\n",
        "eval_df['target'] = target\n",
        "eval_df['kmn_3_label'] = kmn_3_labels\n",
        "#eval_df['kmn_4_label'] = kmn_4_labels\n",
        "eval_df['kmn_5_label'] = kmn_5_labels\n",
        "eval_df['kmn_6_label'] = kmn_6_labels"
      ],
      "metadata": {
        "id": "D2wAezGu5Msl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can execute across different values of k to determine differences in clusters\n",
        "summary_stats, target_groups = group_by_cluster(eval_df, kmn_3_labels)\n",
        "summary_stats"
      ],
      "metadata": {
        "id": "HdN1Fl915PLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at distribution of target variable in different clusters created\n",
        "\n",
        "target_groups"
      ],
      "metadata": {
        "id": "XCz1zMfZ5Vs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OMP_NUM_THREADS=1"
      ],
      "metadata": {
        "id": "qBz8hOs35aD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_execution(df, num_clust):\n",
        "    # Create models & store labels for different number of clusters\n",
        "    kmn = KMeans(n_clusters = num_clust, n_init='auto', random_state = 0) # Create KMeans object\n",
        "    kmn.fit(df) # Apply to the data\n",
        "    kmn_lbl = kmn.labels_ # Capture K-Means labels\n",
        "    kmn_distortion = kmn.inertia_ # Used for elbow method\n",
        "    return kmn, kmn_lbl, kmn_distortion\n",
        "\n",
        "#kmn_2_mod, kmn_2_labels, kmn_2_dist = kmeans_execution(scaled_df, 2)\n",
        "#silhouette_2 = silhouette_score(scaled_df, kmn_2_labels)\n",
        "#print(f'k=2 silhouette average score: {silhouette_2}')\n",
        "kmn_3_mod, kmn_3_labels, kmn_3_dist = kmeans_execution(scaled_df, 3)\n",
        "silhouette_3 = silhouette_score(scaled_df, kmn_3_labels)\n",
        "print(f'k=3 silhouette average score: {silhouette_3}')\n",
        "kmn_4_mod, kmn_4_labels, kmn_4_dist = kmeans_execution(scaled_df, 4)\n",
        "silhouette_4 = silhouette_score(scaled_df, kmn_4_labels)\n",
        "print(f'k=4 silhouette average score: {silhouette_4}')\n",
        "kmn_5_mod, kmn_5_labels, kmn_5_dist = kmeans_execution(scaled_df, 5)\n",
        "silhouette_5 = silhouette_score(scaled_df, kmn_5_labels)\n",
        "#print(f'k=5 silhouette average score: {silhouette_5}')\n",
        "#kmn_6_mod, kmn_6_labels, kmn_6_dist = kmeans_execution(scaled_df, 6)\n",
        "#silhouette_6 = silhouette_score(scaled_df, kmn_6_labels)\n",
        "#print(f'k=6 silhouette average score: {silhouette_6}')"
      ],
      "metadata": {
        "id": "vzXrSQkU5ht9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elbow Plot - seems like clear \"elbow\" at k=\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "#plt.plot(range(2, 7), [kmn_2_dist, kmn_3_dist, kmn_4_dist, kmn_5_dist, kmn_6_dist])\n",
        "plt.plot(range(3, 6), [kmn_3_dist, kmn_4_dist, kmn_5_dist])\n",
        "plt.grid(True)\n",
        "plt.title('Elbow curve')"
      ],
      "metadata": {
        "id": "Wt6wgpY_5rLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create evaluation dataset\n",
        "\n",
        "eval_df = df.copy()\n",
        "eval_df['target'] = target\n",
        "eval_df['kmn_3_label'] = kmn_3_labels\n",
        "eval_df['kmn_4_label'] = kmn_4_labels\n",
        "eval_df['kmn_5_label'] = kmn_5_labels\n",
        "#eval_df['kmn_6_label'] = kmn_6_labels"
      ],
      "metadata": {
        "id": "m3ihzqOC5xzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Create a k-means model with the assignment dataset using at least 10 features.\n",
        "See above.\n",
        "    \n",
        "2) Experiment with at least 3 k values. Be sure to transform variables into the appropriate format before modeling.\n",
        "Note that a larger k will increase the overhead of interpretation, so it is suggested to keep the k less than 10.\n",
        "    a) What transformations did you apply to the raw dataset?\n",
        "    Replaced missing data with zeroes.\n",
        "    Converted enumerations to integers.\n",
        "    Reduced 122 features down to 14 (removed columns)\n",
        "    Scaled the remaining columns.\n",
        "        \n",
        "    b) What were different k's chosen? What were the differences in the output with those different k's\n",
        "    I tried 2 different groupings (3,4,6) and (3,4,5).  In the first case I got the highest silhouette score at 3 and the second case I got the\n",
        "    highest score at 4.  In both cases the graph elbow was at 4.\n",
        "    \n",
        "3) Choose a final k that you think reflects the data the best and provide a written interpretation of the different clusters generated by k-means\n",
        "   \n",
        "    a) Why did you choose this k and distance metric?\n",
        "    I believe k = 5 is the best because the highest Means were achieved for Income, Credit and annuity.\n",
        "\n",
        "    b) Why does it appear these groups have been created?\n",
        "    I think these three features show the ability to take and handle debt; more so than the other ~120 features.\n",
        "        \n",
        "    c) What are the influential features?\n",
        "    I determined the influencial features ti be income, profession, loan size, employment status, marital status, propery ownership, and credit rating.\n",
        "        \n",
        "    d) Are there any inferences you can draw that would be relevant from a business context about the different groups?\n",
        "    Out of all the graphical data pairings, only Goods Price vs Credit had a cleanly linear relationship.  There I think credit is the key indicator\n",
        "    that a loan officer should focus on.  \n"
      ],
      "metadata": {
        "id": "r2lDyhM755Ey"
      }
    }
  ]
}